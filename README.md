# INSPECT-SR Framework: Large Language Model Assessment of RCT Trustworthiness

## ğŸ¯ Overview

The INSPECT-SR (INtegrity and Systematic Review Assessment) Framework is a comprehensive methodology for evaluating the trustworthiness of Randomized Controlled Trials (RCTs) using Large Language Models (LLMs) in systematic reviews.

## ğŸ”¬ Research Purpose

This framework addresses the critical need for efficient, reliable assessment of study trustworthiness in systematic reviews by:
- Comparing manual expert assessment (gold standard) with LLM performance
- Evaluating agreement levels using Cohen's Îº and linear-weighted Îº
- Assessing effect sizes using CramÃ©r's V
- Providing systematic review methodology for AI-assisted quality assessment

## ğŸ“Š Key Components

### Primary Assessment Checks
- **Retraction Detection**: Identification of retracted studies
- **Expression of Concern**: Detection of published concerns
- **Team Integrity**: Assessment of author team credibility
- **Registration Timing**: Evaluation of trial registration compliance

### LLM Models Evaluated
- Claude Sonnet 4
- ChatGPT 5
- Gemini 2.5 Pro

## ğŸš€ Quick Start

### Prerequisites
```r
# Required R packages
install.packages(c(
  "readxl", "dplyr", "tidyr", "purrr", "stringr", "tibble",
  "ggplot2", "boot", "scales", "patchwork", "gt", "kableExtra",
  "openxlsx", "viridis", "sysfonts", "showtext", "ragg",
  "webshot2", "rsvg", "readr", "svglite", "ggrepel"
))
```

### Basic Usage
```r
# Run the complete analysis
source("final.R")

# Convert JSON responses to CSV (if needed)
source("json_to_csv.R")
```

## ğŸ“ Repository Structure

```
Analysis/
â”œâ”€â”€ final.R                          # Main analysis script
â”œâ”€â”€ json_to_csv.R                    # JSON to CSV converter
â”œâ”€â”€ enhanced_analysis.R              # Enhanced analysis functions
â”œâ”€â”€ INSPECT_SR Checks Gagan.xlsx     # Sample data
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ results/                         # Generated outputs
    â”œâ”€â”€ comprehensive_publication_workbook.xlsx
    â”œâ”€â”€ BEST_publication_table.tex
    â”œâ”€â”€ enhanced_summary.csv
    â””â”€â”€ ...
```

## ğŸ“ˆ Outputs Generated

### Professional Visualizations
- **Figure 1**: Binary Check Agreement Analysis (Cohen's Îº)
- **Figure 2**: Ordinal Check Agreement Analysis (Linear-Weighted Îº)
- **Figure 3**: Effect Size Analysis (CramÃ©r's V)
- **Figure 5**: Sample Characteristics Overview
- **Figure 6A**: Manual Gold Standard Assessment
- **Figure 6B**: LLM Performance Evaluation

### Publication-Ready Tables
- LaTeX tables for journal submission
- HTML tables for quick viewing
- Comprehensive Excel workbook for reviewers

### Data Files
- Enhanced summary statistics
- Binary diagnostic metrics
- Missing data patterns
- Session information for reproducibility

## ğŸ”§ Configuration

### Input Data Format
The framework expects Excel data with columns for:
- Manual assessment results (gold standard)
- LLM assessment results
- Study identifiers and metadata

### Output Customization
- Adjust plot dimensions for different journal requirements
- Modify color schemes and themes
- Customize agreement level thresholds

## ğŸ“š Methodology

### Statistical Analysis
- **Cohen's Îº**: For binary agreement assessment
- **Linear-weighted Îº**: For ordinal agreement assessment
- **CramÃ©r's V**: For effect size analysis
- **Bootstrap confidence intervals**: For robust statistical inference

### Quality Assessment
- Agreement level thresholds: Excellent (â‰¥0.80), Good (â‰¥0.60), Fair (â‰¥0.40), Poor (â‰¥0.20)
- Effect size interpretation: Large (â‰¥0.5), Medium (â‰¥0.3), Small (â‰¥0.1)

## ğŸ¨ Design Features

- Professional publication-ready visualizations
- Consistent color schemes and typography
- High-resolution exports (300 DPI)
- Word-optimized dimensions and proportions
- Comprehensive captions and annotations

## ğŸ“– Citation

If you use this framework in your research, please cite:

```
Dhaliwal, G. (2024). INSPECT-SR Framework: Large Language Model Assessment 
of RCT Trustworthiness in Systematic Reviews. [GitHub Repository]
```

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ”— Related Work

- Systematic Review Methodology
- AI-Assisted Research Assessment
- Research Integrity Evaluation
- Evidence Synthesis Automation

## ğŸ“ Contact

- **Author**: Gagan Dhaliwal
- **Email**: [Your Email]
- **GitHub**: [Your GitHub Username]

## ğŸ™ Acknowledgments

- Systematic review methodology community
- Open-source R package developers
- Research integrity researchers

---

**Note**: This framework is designed for research and educational purposes. Always validate results and use appropriate statistical methods for your specific research questions.
